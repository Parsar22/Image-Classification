{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26hIPGtBsi7g"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The goal of this analysis is to develop an image classification algorithm that can classify images into one of three categories: T-shirt/top, Sneaker, and Bag. By leveraging a subset of the Fashion MNIST dataset, we aim to build and evaluate a deep learning model using MobileNetV2 for this classification task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdRLvqUbJyr8"
      },
      "source": [
        "## Data Source\n",
        "\n",
        "The dataset used for this analysis is the Fashion MNIST dataset, which is publicly available and can be accessed through TensorFlow Datasets. Fashion MNIST consists of 70,000 grayscale images in 10 categories, with 7,000 images per category. For this analysis, we will use a subset of the dataset that includes three categories: T-shirt/top, Sneaker, and Bag. More information about the dataset can be found in Tensorflow_Datasets library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihryuR7teh2E"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GM8gg-lhenM8"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BEXJM5BezHM"
      },
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "First, we load and normalize the Fashion MNIST dataset. We filter the dataset to include only three categories: T-shirt/top, Sneaker, and Bag. We also convert the grayscale images to RGB, resize them, and apply data augmentation techniques to enhance the training process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cKDuLsyufMrq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\PARSA\\tensorflow_datasets\\fashion_mnist\\3.0.1...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\PARSA\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
            "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.00 url/s]\n",
            "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  1.91 url/s]\n",
            "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  1.88 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.95 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.85 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.82 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:01<00:01,  1.39 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:01<00:01,  1.24 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:01<00:01,  1.19 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:01<00:01,  1.11 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:01<00:01,  1.07 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:01<00:01,  1.04 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:01<00:01,  1.01 url/s]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.05s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.15s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.16s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.20s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.21s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.28s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.31s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.44s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.44s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.45s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:03<00:03,  1.51s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:03<00:03,  1.61s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:03<00:03,  1.74s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:03<00:03,  1.82s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:03<00:03,  1.83s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:03<00:03,  1.84s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:03<00:03,  1.91s/ url]\n",
            "Dl Completed...:  50%|█████     | 2/4 [00:04<00:04,  2.02s/ url]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:04<00:01,  1.36s/ url]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:04<00:01,  1.36s/ url]\n",
            "\u001b[A\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:04<00:01,  1.36s/ url]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:06<00:01,  1.36s/ url]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:10<00:01,  1.36s/ url]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:11<00:01,  1.36s/ url]\n",
            "Dl Completed...:  75%|███████▌  | 3/4 [00:12<00:01,  1.36s/ url]\n",
            "Dl Completed...: 100%|██████████| 4/4 [00:12<00:00,  3.67s/ url]\n",
            "Dl Completed...: 100%|██████████| 4/4 [00:12<00:00,  3.67s/ url]\n",
            "\u001b[A\n",
            "Dl Completed...: 100%|██████████| 4/4 [00:12<00:00,  3.67s/ url]\n",
            "Extraction completed...: 100%|██████████| 4/4 [00:12<00:00,  3.19s/ file]\n",
            "Dl Size...: 100%|██████████| 29/29 [00:12<00:00,  2.28 MiB/s]\n",
            "Dl Completed...: 100%|██████████| 4/4 [00:12<00:00,  3.19s/ url]\n",
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset fashion_mnist downloaded and prepared to C:\\Users\\PARSA\\tensorflow_datasets\\fashion_mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "(train_ds, validation_ds, test_ds), ds_info = tfds.load(\n",
        "    \"fashion_mnist\",\n",
        "    split=[\"train[:10%]\", \"train[10%:12%]\", \"train[12%:15%]\"],\n",
        "    as_supervised=True,  # Include labels\n",
        "    with_info=True,\n",
        ")\n",
        "def filter_classes(image, label):\n",
        "    return tf.math.logical_or(tf.math.equal(label, 0),\n",
        "                              tf.math.logical_or(tf.math.equal(label, 7),\n",
        "                                                 tf.math.equal(label, 8)))\n",
        "train_ds = train_ds.filter(filter_classes)\n",
        "validation_ds = validation_ds.filter(filter_classes)\n",
        "test_ds = test_ds.filter(filter_classes)\n",
        "\n",
        "# Normalize the images to [0, 1] range\n",
        "def normalize_img(image, label):\n",
        "    return tf.cast(image, tf.float32) / 255.0, label\n",
        "\n",
        "train_ds = train_ds.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "validation_ds = validation_ds.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2r-VEzupHoA"
      },
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "Next, we resize the images to 224x224 pixels, which is the input size expected by the MobileNetV2 model. We also apply data augmentation techniques such as random horizontal flipping and rotation to enhance the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Wm_DNsVgpKla"
      },
      "outputs": [],
      "source": [
        "def convert_to_rgb(image, label):\n",
        "    image = tf.image.grayscale_to_rgb(image)\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(convert_to_rgb, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "validation_ds = validation_ds.map(convert_to_rgb, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.map(convert_to_rgb, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kbyVWcZ6i70x"
      },
      "outputs": [],
      "source": [
        "resize_fn = layers.Resizing(224, 224)\n",
        "\n",
        "# Augmentation layers\n",
        "augmentation_layers = [\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "]\n",
        "\n",
        "\n",
        "def data_augmentation(x, y):\n",
        "    x = resize_fn(x)  # Add resizing inside data_augmentation\n",
        "    for layer in augmentation_layers:\n",
        "        x = layer(x)\n",
        "    return x, y\n",
        "\n",
        "train_ds = train_ds.map(data_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Ensure validation and test datasets are resized\n",
        "validation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.map(lambda x, y: (resize_fn(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ALNLHr-VpY9x"
      },
      "outputs": [],
      "source": [
        "from tensorflow import data as tf_data\n",
        "batch_size = 64  \n",
        "\n",
        "train_ds = train_ds.batch(batch_size).prefetch(1).cache()  # Prefetch 1 batch at a time\n",
        "validation_ds = validation_ds.batch(batch_size).prefetch(1).cache()\n",
        "test_ds = test_ds.batch(batch_size).prefetch(1).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNm5tGrEKTx1"
      },
      "source": [
        "## Model Building and Training\n",
        "\n",
        "We use the MobileNetV2 architecture, which is pre-trained on the ImageNet dataset. We customize the model to classify images into three categories: T-shirt/top, Sneaker, and Bag. We first train only the top layer of the model, keeping the pre-trained layers frozen. Then, we fine-tune the entire model with a low learning rate.\n",
        "\n",
        "### Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqwnTaQppfDn",
        "outputId": "4a99bcd1-b2a4-4442-d9f1-6b619cc4312e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m12,810\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,270,794</span> (8.66 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,270,794\u001b[0m (8.66 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> (50.04 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,810\u001b[0m (50.04 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting the top layer of the model\n",
            "Epoch 1/5\n",
            "     28/Unknown \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.2917 - loss: 1.4301"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\PARSA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 2s/step - accuracy: 0.2923 - loss: 1.4244 - val_accuracy: 0.3585 - val_loss: 1.1094\n",
            "Epoch 2/5\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.3365 - loss: 1.1706 - val_accuracy: 0.3585 - val_loss: 1.0851\n",
            "Epoch 3/5\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.3174 - loss: 1.1710 - val_accuracy: 0.3585 - val_loss: 1.0775\n",
            "Epoch 4/5\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.3568 - loss: 1.1337 - val_accuracy: 0.3585 - val_loss: 1.0570\n",
            "Epoch 5/5\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.3637 - loss: 1.1117 - val_accuracy: 0.3585 - val_loss: 1.0441\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build the model\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "scale_layer = layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
        "x = scale_layer(inputs)\n",
        "\n",
        "x = base_model(x, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.2)(x)  \n",
        "outputs = layers.Dense(10)(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "\n",
        "epochs = 5  \n",
        "print(\"Fitting the top layer of the model\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_ds,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wl-qD02KZ-u"
      },
      "source": [
        "After training the top layer, we proceed to fine-tune the entire MobileNetV2 model. Fine-tuning involves unfreezing the base model and training it end-to-end with a very low learning rate. This allows the weights of the pre-trained model to be slightly adjusted to better fit our specific dataset.\n",
        "\n",
        "By fine-tuning, we aim to improve the model's performance by leveraging the rich features learned by the MobileNetV2 model during its pre-training on the ImageNet dataset, while adapting it to our specific task of classifying T-shirt/top, Sneaker, and Bag images from the Fashion MNIST dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "id": "okL722AUKtrZ",
        "outputId": "3b2d55fa-951a-4b30-983c-97bdf4828da8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m12,810\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,270,794</span> (8.66 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,270,794\u001b[0m (8.66 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,236,682</span> (8.53 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,236,682\u001b[0m (8.53 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,112</span> (133.25 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m34,112\u001b[0m (133.25 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting the end-to-end model\n",
            "Epoch 1/5\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 7s/step - accuracy: 0.3474 - loss: 1.9285 - val_accuracy: 0.3585 - val_loss: 1.0980\n",
            "Epoch 2/5\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 6s/step - accuracy: 0.7015 - loss: 0.6682 - val_accuracy: 0.5714 - val_loss: 1.1059\n",
            "Epoch 3/5\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 6s/step - accuracy: 0.9046 - loss: 0.3143 - val_accuracy: 0.3558 - val_loss: 1.1550\n",
            "Epoch 4/5\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 6s/step - accuracy: 0.9439 - loss: 0.2171 - val_accuracy: 0.3558 - val_loss: 1.3091\n",
            "Epoch 5/5\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 6s/step - accuracy: 0.9614 - loss: 0.1675 - val_accuracy: 0.3558 - val_loss: 1.4460\n"
          ]
        }
      ],
      "source": [
        "# Fine-tune the entire model\n",
        "base_model.trainable = True\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),  # Low learning rate\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "print(\"Fitting the end-to-end model\")\n",
        "history_fine = model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_ds\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation of Results\n",
        "\n",
        "From the fine-tuning results, we observe that the training accuracy improved significantly, reaching over 96%, which is an excellent result. However, the validation accuracy did not improve correspondingly and remained around 35.58%. This discrepancy indicates that while the model performs exceptionally well on the training data, it does not generalize as well to the validation data.\n",
        "\n",
        "In conclusion, the model shows strong learning capability on the training data, but additional efforts are needed to improve its performance on unseen data.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
